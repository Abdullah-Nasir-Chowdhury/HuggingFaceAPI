{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9961605072021484}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier('I suck at coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662,  2227, 14924,\n",
      "          4818,  2026,  2878,  2166,  1010,  1045,  2572,  2061,  3407,   999,\n",
      "           102],\n",
      "        [  101,  1045,  2572,  2061,  9364,  1999,  2026,  3754,  2000,  4553,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([2, 21, 768])\n",
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.4223,  0.2644,  0.2841,  ...,  0.5133,  0.7970, -0.5389],\n",
      "         [ 0.6034,  0.4271,  0.2106,  ...,  0.5094,  0.8745, -0.4014],\n",
      "         [ 0.6883,  0.5008,  0.2713,  ...,  0.4099,  0.7448, -0.1239],\n",
      "         ...,\n",
      "         [ 0.5705,  0.3254,  0.1810,  ...,  0.5382,  0.7913, -0.5631],\n",
      "         [ 0.5371,  0.2903,  0.1535,  ...,  0.5578,  0.8199, -0.4776],\n",
      "         [ 1.2266,  0.2534,  0.4621,  ...,  0.7747,  0.5059, -0.8307]],\n",
      "\n",
      "        [[-0.7461,  0.9088, -0.0971,  ...,  0.1799, -0.9920, -0.4135],\n",
      "         [-0.7748,  0.9645, -0.0093,  ..., -0.0285, -0.9143, -0.1535],\n",
      "         [-0.8590,  0.9238, -0.0158,  ...,  0.0062, -1.0241, -0.1349],\n",
      "         ...,\n",
      "         [-0.6346,  0.9681, -0.0236,  ...,  0.1793, -1.1010, -0.2452],\n",
      "         [-0.5911,  0.9420, -0.1765,  ...,  0.2015, -1.0720, -0.2666],\n",
      "         [-0.5166,  0.9548, -0.1337,  ...,  0.2211, -1.0757, -0.2626]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[-4.2574,  4.6149],\n",
      "        [ 4.6649, -3.7399]], grad_fn=<AddmmBackward0>)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2574,  4.6149],\n",
      "        [ 4.6649, -3.7399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[1.4020e-04, 9.9986e-01],\n",
      "        [9.9978e-01, 2.2374e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2)\n",
      "2\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "raw_inputs = ['''I have been waiting for a hugging face tutorial my whole life, i am so happy!''',\n",
    "              'I am so disappointed in my ability to learn']\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)\n",
    "print(outputs)\n",
    "\n",
    "import torch\n",
    "torch_inputs = outputs.logits\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(probabilities)\n",
    "\n",
    "# check output rating:\n",
    "print(torch.argmax(outputs.logits))\n",
    "print(int(torch.argmax(outputs.logits)))\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
